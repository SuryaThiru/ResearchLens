{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    load_index_from_storage,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "# needed to synthesize responses later\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(\"/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.cohere import Cohere\n",
    "\n",
    "cohere_model = Cohere(api_key=\"vORtxj32na8zl2ceIbxH1c5tNziAVWDdAy2x3sbX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter\n",
    "Settings.llm = cohere_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-using index from RAG_doc notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    persist_dir=\"./storage\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1500)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    llm=cohere_model,\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as explaining research papers.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "    ),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Based on the above documents, provide a detailed answer for the user question below.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"\"\"\n",
    "StarCoder [15]\n",
    " 15.6B\n",
    " 6.3\n",
    " 4.1\n",
    " 0.7\n",
    " 4.7\n",
    "CodeLlama-Instruct [27]\n",
    " 13B\n",
    " 33.3\n",
    " 11.0\n",
    " 1.4\n",
    " 18.7\n",
    "WizardCoder-Python-V1.0 [23]\n",
    " 13B\n",
    " 39.7\n",
    " 15.1\n",
    " 4.3\n",
    " 23.6\n",
    "DeepSeek-Coder-Instruct [8]\n",
    " 6.7B\n",
    " 49.4\n",
    " 18.7\n",
    " 3.6\n",
    " 29.2\n",
    "SFT on APPS+\n",
    "\n",
    "How is the DeepSeek-Coder-Instruct model related to the StepCoder paper?\n",
    "What was the training dataset that was used to train DeepSeek-Coder-Instruct model?            \n",
    "\"\"\".strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='aba63e77-2649-48a0-becb-4c7a26182e5e', embedding=None, metadata={'page_label': '6', 'file_name': 'Stepcoder.pdf', 'file_path': '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs/Stepcoder.pdf', 'file_type': 'application/pdf', 'file_size': 652507, 'creation_date': '2024-04-04', 'last_modified_date': '2024-04-04'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a46cc157-0b06-4a22-870b-aacdb1b60185', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'Stepcoder.pdf', 'file_path': '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs/Stepcoder.pdf', 'file_type': 'application/pdf', 'file_size': 652507, 'creation_date': '2024-04-04', 'last_modified_date': '2024-04-04'}, hash='55b72484046d331e7c87517ab2ede07d1b4d368a17b87d6c9c18643a84f0ed7b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c2e2d616-7b60-46e1-84ba-5a9d05b59f68', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '5', 'file_name': 'Stepcoder.pdf', 'file_path': '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs/Stepcoder.pdf', 'file_type': 'application/pdf', 'file_size': 652507, 'creation_date': '2024-04-04', 'last_modified_date': '2024-04-04'}, hash='b2e11da6ee465a28200243b67f0ffda95b16d225fc3d8436845831619b50e3b0'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='76d755e7-71d2-4832-8dcf-c8ab7555214b', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='0585eff926231b30bf4c0a81fa79c4eb189dd344dcd12672cb40e585c23dd366')}, text='Models SizeAPPS+\\nIntroductory Interview Competition Overall\\nBase Models\\nCodeLlama [27] 13B 18.7 11.0 0.0 13.0\\nCodeLlama-Python [27] 13B 29.0 12.3 2.9 17.9\\nDeepSeek-Coder-Base [8] 6.7B 13.0 10.3 5.0 10.9\\nSupervised Fine-tuned Models\\nStarCoder [15] 15.6B 6.3 4.1 0.7 4.7\\nCodeLlama-Instruct [27] 13B 33.3 11.0 1.4 18.7\\nWizardCoder-Python-V1.0 [23] 13B 39.7 15.1 4.3 23.6\\nDeepSeek-Coder-Instruct [8] 6.7B 49.4 18.7 3.6 29.2\\nSFT on APPS+ 6.7B 50.1 19.0 6.4 29.8\\nReinforcement Learning-based Models (Using DeepSeek-Coder-Instruct-6.7B as the backbone)\\nVanilla PPO 6.7B 53.7 20.1 5.0 31.7\\nPPOCoder [33] 6.7B 54.4 20.3 6.4 32.1\\nRLTF [20] 6.7B 55.1 20.8 6.4 32.7\\nStepCoder (Ours) 6.7B 59.7 23.5 8.6 36.1\\nw/oCCCS 6.7B 58.7 21.7 7.1 34.6\\nw/oFGO 6.7B 58.4 23.3 8.6 35.5\\nTable 1: Results of pass@1 on our proposed APPS+. We compare popular and widely used state-of-the-art methods\\nwith our method.', start_char_idx=0, end_char_idx=884, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6779357099598663),\n",
       " NodeWithScore(node=TextNode(id_='76d755e7-71d2-4832-8dcf-c8ab7555214b', embedding=None, metadata={'page_label': '6', 'file_name': 'Stepcoder.pdf', 'file_path': '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs/Stepcoder.pdf', 'file_type': 'application/pdf', 'file_size': 652507, 'creation_date': '2024-04-04', 'last_modified_date': '2024-04-04'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='a46cc157-0b06-4a22-870b-aacdb1b60185', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '6', 'file_name': 'Stepcoder.pdf', 'file_path': '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs/Stepcoder.pdf', 'file_type': 'application/pdf', 'file_size': 652507, 'creation_date': '2024-04-04', 'last_modified_date': '2024-04-04'}, hash='55b72484046d331e7c87517ab2ede07d1b4d368a17b87d6c9c18643a84f0ed7b'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='aba63e77-2649-48a0-becb-4c7a26182e5e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '6', 'file_name': 'Stepcoder.pdf', 'file_path': '/home/surya/NEU/CS5100 FAI/Project/ResearchLens/experiments/RAG/pdfs/Stepcoder.pdf', 'file_type': 'application/pdf', 'file_size': 652507, 'creation_date': '2024-04-04', 'last_modified_date': '2024-04-04'}, hash='8f4a4cd6952789820b6c31e1d7825d2a50a3cb9ee7ccd4918b111a5fab204453'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='282ba6ac-26ec-400b-945b-d323bf59c86f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='00e22e5589f7f82ea14ceb7b4badba33bad545d3cce7e2fedf0ac2e2e4371cae')}, text='To ensure a fair comparison, we apply these RL-based methods using the same base model (i.e.,\\nDeepSeek-Coder-Instruct-6.7B [ 8]) as a backbone on the APPS+ dataset. In addition, We conduct supervised\\nfine-tuning using our APPS+ dataset based on DeepSeek-Coder-Instruct-6.7B to further validate the effectiveness\\nand necessity of our approach.\\n[23], DeepSeek-Coder [ 8], and three versions of\\nCodeLlama (Base, Python, Instruct) [ 27]. More-\\nover, we also consider vanilla PPO and two state-of-\\nthe-art RL-based approaches, including PPOCoder\\n[33] and RLTF [ 20]. We carried out experiments\\napplying these methods utilizing the same back-\\nbone (i.e., DeepSeek-Coder-Instruct [ 8]) on the\\nAPPS+ dataset to ensure a fair comparison. In\\naddition to demonstrating the necessity and effec-\\ntiveness of our method, we also supervised fine-\\ntuning DeepSeek-Coder-Instruct [ 8] on the APPS+\\ndataset to exclude the effect of training data. The\\ndetailed description of these baselines is discussed\\nin Appendix B.2.\\nImplementation Details. During the SFT phase,\\nwe adopt a learning rate set at 2e−5, conduct train-\\ning for three epochs, and employ a warm-up period\\nof0.3epochs, with a linear decay to zero. The fine-\\ntuning process was conducted on a device with\\neight NVIDIA A100 80G GPUs, with the global\\nbatch size set to 64. In the PPO training phase,\\nwe employ a learning rate of 5e−7for the policy\\nmodel and 1.5e−6for the critic model. For each ex-\\nample, we collect a 16roll-out code using nucleus\\nsampling. The sampling temperature is set to 0.8,\\ntop-p is set to 0.9, and the maximum output tokenlength is set to 1024 . The token-level KL penalty\\ncoefficient βis set to 0.05, with a clip value of 0.8.', start_char_idx=885, end_char_idx=2581, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.6090179616537649)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.source_nodes[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chat in chat_engine.chat_history:\n",
    "#     print(chat.role)\n",
    "#     print(chat.content)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek-Coder-Instruct is one of the well-known base models for code completion, and it was utilized as a backbone in many reinforcement learning approaches for code completion, including StepCoder. \n",
      "\n",
      "The DeepSeek-Coder-Instruct model is a neural language model that has been fine-tuned on the APPS+ dataset, which is a dataset of 28 million functions and snippets of code. The model was fine-tuned using a supervised learning approach, where the model was trained to predict the next token in a sequence of tokens, with the goal of generating functional code. \n",
      "\n",
      "The StepCoder model is a reinforcement learning approach for code completion that utilized the DeepSeek-Coder-Instruct model as a backbone and was trained on the APPS+ dataset. The StepCoder model used a combination of reinforcement learning and supervised learning to produce more efficient and cleaner code outputs.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
