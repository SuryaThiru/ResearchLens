{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "769c0bc8-a403-4b0b-b0e6-037ebe05c7eb",
   "metadata": {},
   "source": [
    "# Citation Extractor\n",
    "\n",
    "Given a pdf file, an extract of text in the file, fetch all the papers cited in the extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d893704-9eb8-4cba-ac13-cdbb337b5da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "from rapidfuzz import fuzz\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "606a129a-3199-4473-a4ad-4ad86538d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"/home/surya/NEU/CS5100 FAI/Project/pdfreader/\"\n",
    "file = datadir + \"test3.pdf\"\n",
    "doc = fitz.open(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c24e408f-9f0f-44ea-b0eb-ac7e6b2b945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = \"\"\"\n",
    "This is the same objective optimized in prior works [49, 38, 1, 26] using the DPO-equivalent reward\n",
    "for the reward class of rφ . In this setting, we can interpret the normalization term in f (rφ, πref , β)\n",
    "as the soft value function of the reference policy πref . While this term does not affect the optimal\n",
    "solution, without it, the policy gradient of the objective could have high variance, making learning\n",
    "unstable. We can accommodate for the normalization term using a learned value function, but that\n",
    "can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human\n",
    "completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In\n",
    "contrast the DPO reparameterization yields a reward function that does not require any baselines.\n",
    "\"\"\"\n",
    "\n",
    "extract = extract.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5cfb4e3-011f-408e-9341-a0de57c75683",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract = \"\"\"\n",
    "We outperform state-of-the-arts in multiple datasets, including our novel\n",
    "MPHOI-72 dataset, the single-human HOI CAD-120 [24] dataset, and the two-\n",
    "Multi-person Human-object Interaction Recognition\n",
    " 3\n",
    "hand Bimanual Actions [9] dataset. We also extensively evaluate core compo-\n",
    "nents of 2G-GCN in ablation studies. Our main contributions are as follows:\n",
    "– We propose a novel geometry-informed 2G-GCN network for HOI recog-\n",
    "nition in videos. The network consists of a two-level graph structure that\n",
    "models geometric features between human and object, together with the\n",
    "corresponding visual features.\n",
    "– We present the novel problem of MPHOI in videos with a new MPHOI-72\n",
    "dataset, showcasing new challenges that cannot be directly resolved by ex-\n",
    "isting methods. The source code and dataset are made public1.\n",
    "– We outperform state-of-the-art HOI recognition networks in our MPHOI-72\n",
    "dataset, the CAD-120 [24] dataset and the Bimanual Actions [9] dataset.\n",
    "\"\"\"\n",
    "\n",
    "extract = extract.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d47c5a-7bd6-4bdd-b164-6726eb7b3f34",
   "metadata": {},
   "source": [
    "## Find the extract text in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223749bf-be64-4bf2-957c-9ba40f4f7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "057233a1-da2f-421c-96bd-63129a771f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                         \r"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "\n",
    "for page_num in tqdm(range(len(doc)), leave=False):\n",
    "    page = doc.load_page(page_num)  # load the current page\n",
    "    text_blocks = page.get_text_blocks()  # get a list of links on the current page\n",
    "    for block in text_blocks:\n",
    "        text = block[4]\n",
    "\n",
    "        match_score = fuzz.partial_ratio(extract, text)\n",
    "\n",
    "        if match_score >= THRESHOLD:\n",
    "            matches.append((block, page_num, match_score))\n",
    "\n",
    "matches = sorted(matches, key=lambda x: x[2], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bca365a-5134-4e97-9601-11cb4e9c8bb8",
   "metadata": {},
   "source": [
    "Remove matches that are too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "208bd5af-9d64-4ec4-8d66-23811413b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINTEXTLEN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fe19653-2442-4eda-8b4b-ac65b49be709",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = filter(lambda x: len(x[0][4]) > MINTEXTLEN, matches)\n",
    "matches = list(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c30ff3c-b6f9-4548-b91c-946b208c4768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((134.7650146484375,\n",
       "   118.32608795166016,\n",
       "   480.59674072265625,\n",
       "   140.2436981201172,\n",
       "   'hand Bimanual Actions [9] dataset. We also extensively evaluate core compo-\\nnents of 2G-GCN in ablation studies. Our main contributions are as follows:\\n',\n",
       "   1,\n",
       "   0),\n",
       "  2,\n",
       "  100.0),\n",
       " ((140.9910125732422,\n",
       "   154.87307739257812,\n",
       "   480.6162414550781,\n",
       "   200.7017059326172,\n",
       "   '– We propose a novel geometry-informed 2G-GCN network for HOI recog-\\nnition in videos. The network consists of a two-level graph structure that\\nmodels geometric features between human and object, together with the\\ncorresponding visual features.\\n',\n",
       "   2,\n",
       "   0),\n",
       "  2,\n",
       "  100.0),\n",
       " ((140.9910125732422,\n",
       "   203.22909545898438,\n",
       "   480.6263122558594,\n",
       "   237.1017303466797,\n",
       "   '– We present the novel problem of MPHOI in videos with a new MPHOI-72\\ndataset, showcasing new challenges that cannot be directly resolved by ex-\\nisting methods. The source code and dataset are made public1.\\n',\n",
       "   3,\n",
       "   0),\n",
       "  2,\n",
       "  100.0),\n",
       " ((140.99099731445312,\n",
       "   239.62911987304688,\n",
       "   480.6262512207031,\n",
       "   261.5467529296875,\n",
       "   '– We outperform state-of-the-art HOI recognition networks in our MPHOI-72\\ndataset, the CAD-120 [24] dataset and the Bimanual Actions [9] dataset.\\n',\n",
       "   4,\n",
       "   0),\n",
       "  2,\n",
       "  99.65635738831615),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.64453125,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n13\\n',\n",
       "   0,\n",
       "   0),\n",
       "  12,\n",
       "  98.11320754716981),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.6445007324219,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n3\\n',\n",
       "   0,\n",
       "   0),\n",
       "  2,\n",
       "  98.07692307692307),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.64453125,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n11\\n',\n",
       "   0,\n",
       "   0),\n",
       "  10,\n",
       "  96.22641509433963),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.64453125,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n15\\n',\n",
       "   0,\n",
       "   0),\n",
       "  14,\n",
       "  96.22641509433963),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.64453125,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n17\\n',\n",
       "   0,\n",
       "   0),\n",
       "  16,\n",
       "  96.22641509433963),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.6445007324219,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n5\\n',\n",
       "   0,\n",
       "   0),\n",
       "  4,\n",
       "  96.15384615384616),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.6445007324219,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n7\\n',\n",
       "   0,\n",
       "   0),\n",
       "  6,\n",
       "  96.15384615384616),\n",
       " ((237.4040069580078,\n",
       "   93.17021942138672,\n",
       "   480.6445007324219,\n",
       "   102.1366195678711,\n",
       "   'Multi-person Human-object Interaction Recognition\\n9\\n',\n",
       "   0,\n",
       "   0),\n",
       "  8,\n",
       "  96.15384615384616)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf47a88-4008-4965-b92a-841a0ceca0f3",
   "metadata": {},
   "source": [
    "## Get all citation numbers in the text and the corresponding links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2372ef35-e72f-4bea-9b42-c113bd19022d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_links(matched_block, matched_page):\n",
    "    # get the matched region\n",
    "    matched_bbox = fitz.Rect(matched_block[:4])\n",
    "\n",
    "    # get the citation links\n",
    "    matched_links = []\n",
    "    \n",
    "    for link in doc[matched_page].get_links():\n",
    "        if link['kind'] == 4:   # internal links\n",
    "            link_bbox = link['from']\n",
    "            if matched_bbox.intersects(link_bbox):\n",
    "                link['from_page'] = matched_page\n",
    "                matched_links.append(link)\n",
    "\n",
    "    return matched_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50081ecb-cf80-4c0e-92f1-e981a494884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_links = []\n",
    "\n",
    "for match in matches:\n",
    "    matched_links.extend(get_matching_links(match[0], match[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33d396ac-4ce2-41f2-bccb-da941eed7b12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kind': 4,\n",
       "  'xref': 190,\n",
       "  'from': Rect(245.1490020751953, 118.38201904296875, 252.1230010986328, 126.79498291015625),\n",
       "  'page': 14,\n",
       "  'to': Point(134.765, 349.735),\n",
       "  'zoom': 0.0,\n",
       "  'nameddest': 'cite.dreher2020learning',\n",
       "  'id': '',\n",
       "  'from_page': 2},\n",
       " {'kind': 4,\n",
       "  'xref': 191,\n",
       "  'from': Rect(418.8659973144531, 225.5050048828125, 425.3280029296875, 237.54498291015625),\n",
       "  'page': 2,\n",
       "  'to': Point(144.727, 140.037),\n",
       "  'zoom': 0.0,\n",
       "  'nameddest': 'Hfootnote.1',\n",
       "  'id': '',\n",
       "  'from_page': 2},\n",
       " {'kind': 4,\n",
       "  'xref': 192,\n",
       "  'from': Rect(252.18600463867188, 251.6400146484375, 264.1409912109375, 260.052001953125),\n",
       "  'page': 15,\n",
       "  'to': Point(134.765, 418.009),\n",
       "  'zoom': 0.0,\n",
       "  'nameddest': 'cite.koppula2013learning',\n",
       "  'id': '',\n",
       "  'from_page': 2},\n",
       " {'kind': 4,\n",
       "  'xref': 193,\n",
       "  'from': Rect(423.73699951171875, 251.6400146484375, 430.71099853515625, 260.052001953125),\n",
       "  'page': 14,\n",
       "  'to': Point(134.765, 349.735),\n",
       "  'zoom': 0.0,\n",
       "  'nameddest': 'cite.dreher2020learning',\n",
       "  'id': '',\n",
       "  'from_page': 2}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa7efb-ffdf-45cb-af0a-17f2b8b26c9a",
   "metadata": {},
   "source": [
    "Get citation numbers for each each link.\n",
    "\n",
    "Here we also filter out the citation links that are not part of the original extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7946ab2b-c11e-475b-9e45-ad340db58dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind': 4, 'xref': 190, 'from': Rect(245.1490020751953, 118.38201904296875, 252.1230010986328, 126.79498291015625), 'page': 14, 'to': Point(134.765, 349.735), 'zoom': 0.0, 'nameddest': 'cite.dreher2020learning', 'id': '', 'from_page': 2}\n",
      "[9]\n",
      "\n",
      "{'kind': 4, 'xref': 192, 'from': Rect(252.18600463867188, 251.6400146484375, 264.1409912109375, 260.052001953125), 'page': 15, 'to': Point(134.765, 418.009), 'zoom': 0.0, 'nameddest': 'cite.koppula2013learning', 'id': '', 'from_page': 2}\n",
      "[24]\n",
      "\n",
      "{'kind': 4, 'xref': 193, 'from': Rect(423.73699951171875, 251.6400146484375, 430.71099853515625, 260.052001953125), 'page': 14, 'to': Point(134.765, 349.735), 'zoom': 0.0, 'nameddest': 'cite.dreher2020learning', 'id': '', 'from_page': 2}\n",
      "[9]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matched_links_filtered = []\n",
    "\n",
    "# page = doc[matched_page]\n",
    "for link in matched_links:\n",
    "    # keep only citations, not equations and figures\n",
    "    if not link['nameddest'].startswith('cite.'):\n",
    "        continue\n",
    "        \n",
    "    citation_num = doc[link['from_page']].get_text('text', clip=link['from'])\n",
    "    print(link)\n",
    "    print(citation_num)\n",
    "    citation_num = re.findall(r'\\d+', citation_num)\n",
    "\n",
    "    if len(citation_num) == 0:\n",
    "        continue\n",
    "        \n",
    "    citation_num = citation_num[0]\n",
    "\n",
    "    if citation_num not in extract:\n",
    "        continue\n",
    "    \n",
    "    link['citation_number'] = citation_num\n",
    "    matched_links_filtered.append(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d63bbd-3694-413d-9bcc-e102f36c5674",
   "metadata": {},
   "source": [
    "Filter out references that do have a page link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38a0b716-c269-46fa-9a3c-a372ede54918",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_filtered = []\n",
    "for match in matched_links_filtered:\n",
    "    if 'page' not in match:\n",
    "        print(\"Page link not found for\", match[\"citation_number\"])\n",
    "        continue\n",
    "    \n",
    "    new_filtered.append(match)\n",
    "\n",
    "matched_links_filtered = new_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64321e0c-ea96-4abb-9a5f-a81a49d020b3",
   "metadata": {},
   "source": [
    "Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ed365f0-a8c1-41d8-9a7d-ac546d598694",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_matches = {}\n",
    "\n",
    "for match in matched_links_filtered:\n",
    "    citation_num = match['citation_number']\n",
    "\n",
    "    # already exist, duplicate - keep if a link has more attributes than an existing one\n",
    "    if citation_num in unique_matches and len(match.keys()) < len(unique_matches[citation_num]):\n",
    "        continue\n",
    "\n",
    "    # doesn't exist\n",
    "    unique_matches[citation_num] = match\n",
    "\n",
    "matched_links_filtered = list(unique_matches.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fac2d4d-c71c-4dbb-8019-c505904d1539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9', '24']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m['citation_number'] for m in matched_links_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7888f9ca-6d81-4d77-8e3c-efacbf06308d",
   "metadata": {},
   "source": [
    "## Get the references for these citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c695a4-1465-47e3-9952-99db303c33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_references = []\n",
    "\n",
    "for link in matched_links_filtered:\n",
    "    linked_page = doc.load_page(link['page'])\n",
    "    text_blocks = linked_page.get_text(\"blocks\")\n",
    "    citation_num = link['citation_number']\n",
    "    num_pat = r'\\b' + citation_num + r'\\b'\n",
    "    \n",
    "    for text in text_blocks:\n",
    "        # citation number should be present in the initial section of the reference\n",
    "        # if citation_num in text[4][:15]:\n",
    "        if re.search(num_pat, text[4][:15]):\n",
    "            matched_references.append(text[4].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be1c48c9-54d9-47dd-8bf2-0f5971752a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9. Dreher, C.R., W¨ achter, M., Asfour, T.: Learning object-action relations from bi- manual human demonstration using graph networks. IEEE Robotics and Automa- tion Letters 5(1), 187–194 (2020)',\n",
       " '24. Koppula, H.S., Gupta, R., Saxena, A.: Learning human activities and object affor- dances from rgb-d videos. The International Journal of Robotics Research 32(8), 951–970 (2013)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_references = list(map(lambda x: x.replace('\\n', ' '), matched_references))\n",
    "matched_references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009777a-7ac1-4aff-9f72-ae05fdb13542",
   "metadata": {},
   "source": [
    "## Format the references\n",
    "\n",
    "Extract clean attributes from the references. This will make the searches more reliable and accurate.\n",
    "\n",
    "Some references:\n",
    "\n",
    "https://anystyle.io/   - Written in ruby, present as cli and web api.\n",
    "\n",
    "https://pypi.org/project/refextract/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063f365-0a05-4616-8fa7-0e87bb70479b",
   "metadata": {},
   "source": [
    "### anystyle.io\n",
    "\n",
    "To avoid setting up ruby and using the libraries. I had to setup my own simple ruby server locally on docker, with some simple sinatra code.\n",
    "\n",
    "The following section would work once the container is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77568d1d-e9ef-4c7c-8949-ff3e3e7fc895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_from_reftext(reftext, min_title_len=15):\n",
    "    reftext = reftext.encode(\"utf-8\")\n",
    "    response = requests.post('http://localhost:4567/parse', headers={\"Content-Type\": \"text/plain\"},\n",
    "                        data = reftext)\n",
    "    parsed_data = response.json()\n",
    "\n",
    "    title = parsed_data[0]['title']\n",
    "    title = ' '.join(title)\n",
    "\n",
    "    # date = parsed_data[0]['date']\n",
    "    # date = ' '.join(date)\n",
    "    \n",
    "    assert len(title) >= min_title_len\n",
    "\n",
    "    # title = title + \" \" + date\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f2e2b5b-f8cc-43f1-b9da-e08ea22bf119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:05<00:00,  3.83it/s]\n"
     ]
    }
   ],
   "source": [
    "matched_references_title = []\n",
    "\n",
    "for reftext in tqdm(matched_references):\n",
    "    title = get_title_from_reftext(reftext)\n",
    "    matched_references_title.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2596618-efde-4db0-8103-97827ee1ead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaling egocentric vision: The epic-kitchens dataset',\n",
       " 'Ego4d: Around the world in 3,000 hours of egocentric video',\n",
       " 'Discovering important people and objects for egocentric video summarization',\n",
       " 'Detecting activities of daily living in first-person camera views',\n",
       " 'Social interactions: A first-person perspective',\n",
       " 'Epic-fusion: Audio-visual temporal binding for egocentric action recognition',\n",
       " 'Ego-exo: Transferring visual representa- tions from third-person to first-person videos',\n",
       " 'Temporal perception and prediction in ego-centric video',\n",
       " 'What makes training multi-modal classification networks hard?',\n",
       " 'Large-scale weakly-supervised pre-training for video action recognition',\n",
       " 'When will you do what? - anticipating temporal occurrences of activities',\n",
       " 'Rolling-unrolling lstms for action anticipation from first-person video',\n",
       " 'Anticipative video transformer',\n",
       " 'Summarization of egocentric videos: A comprehensive survey',\n",
       " 'Story-driven summarization for egocentric video',\n",
       " 'Epic-kitchens visor benchmark: Video segmentations and object relations',\n",
       " 'Forecasting human object interaction: Joint prediction of motor attention and egocentric activity',\n",
       " 'Seeing invisible poses: Estimating 3d body pose from egocentric video',\n",
       " 'Personal object discovery in first-person videos',\n",
       " 'Is first person vision challenging for object tracking?',\n",
       " 'Visual object tracking in first person vision']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_references_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6cf7c2-1675-4cf3-a671-19bc8a76ea3c",
   "metadata": {},
   "source": [
    "## Get the metadata of these references\n",
    "\n",
    "We will use external services to query for these reference texts and get the relevant metadata.\n",
    "\n",
    "### Observations\n",
    "\n",
    "* using scholarly (which uses google scholar) posed a lot of challenges in networking but worked well, particularly in directing getting the pdf.\n",
    "* using habanero works well for a lot of cases, but fails for a lot of arxiv papers\n",
    "* In a lot of the services, using the wrong year (seems to be common with arxiv - conference mismatches) completely messes up the results\n",
    "* semantic scholar works well, but sometimes can't show pdfs, especially when there is an arxiv paper. I guess pre-prints are not exactly the open-access version of the published paper. But for our purposes it should be good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999c82b-b9ce-49cc-82ea-068d0d4ea7c5",
   "metadata": {},
   "source": [
    "### Semantic scholar\n",
    "\n",
    "I've requested the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40ab6143-aa60-48e1-b234-6dd03df3165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "s2_api_key = 'WWxz8zHVUm6DWzkmw6ZSd3eA94kWbbX46Zl5jR11'\n",
    "sch = SemanticScholar(api_key=s2_api_key, timeout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7790140d-72bc-4b95-afb4-1ac87bea8759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:46<00:00,  2.23s/it]\n"
     ]
    }
   ],
   "source": [
    "matched_references_meta = []\n",
    "\n",
    "for ref in tqdm(matched_references_title):\n",
    "    results = sch.search_paper(ref, limit=1, \n",
    "                               fields=['title', 'paperId', 'externalIds', 'openAccessPdf'])\n",
    "    meta = results[0]\n",
    "    matched_references_meta.append(meta.raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "764ac13f-6913-4199-96df-6f28bfb51576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaling Egocentric Vision: The EPIC-KITCHENS Dataset',\n",
       " 'Ego4D: Around the World in 3,000 Hours of Egocentric Video',\n",
       " 'Discovering important people and objects for egocentric video summarization',\n",
       " 'Detecting activities of daily living in first-person camera views',\n",
       " 'Social interactions: A first-person perspective',\n",
       " 'EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition',\n",
       " 'Ego-Exo: Transferring Visual Representations from Third-person to First-person Videos',\n",
       " 'Temporal Perception and Prediction in Ego-Centric Video',\n",
       " 'What Makes Training Multi-Modal Classification Networks Hard?',\n",
       " 'Large-Scale Weakly-Supervised Pre-Training for Video Action Recognition',\n",
       " 'When will you do what? - Anticipating Temporal Occurrences of Activities',\n",
       " 'Rolling-Unrolling LSTMs for Action Anticipation from First-Person Video',\n",
       " 'Anticipative Video Transformer',\n",
       " 'Summarization of Egocentric Videos: A Comprehensive Survey',\n",
       " 'Story-Driven Summarization for Egocentric Video',\n",
       " 'EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations',\n",
       " 'Forecasting Human Object Interaction: Joint Prediction of Motor Attention and Egocentric Activity',\n",
       " 'Seeing Invisible Poses: Estimating 3D Body Pose from Egocentric Video',\n",
       " 'Personal object discovery in first-person videos',\n",
       " 'Is First Person Vision Challenging for Object Tracking?',\n",
       " 'Visual Object Tracking in First Person Vision']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m['title'] for m in matched_references_meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8701bfc-619c-467a-9176-dd0601e89dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Scaling egocentric vision: The epic-kitchens dataset',\n",
       " 'Ego4d: Around the world in 3,000 hours of egocentric video',\n",
       " 'Discovering important people and objects for egocentric video summarization',\n",
       " 'Detecting activities of daily living in first-person camera views',\n",
       " 'Social interactions: A first-person perspective',\n",
       " 'Epic-fusion: Audio-visual temporal binding for egocentric action recognition',\n",
       " 'Ego-exo: Transferring visual representa- tions from third-person to first-person videos',\n",
       " 'Temporal perception and prediction in ego-centric video',\n",
       " 'What makes training multi-modal classification networks hard?',\n",
       " 'Large-scale weakly-supervised pre-training for video action recognition',\n",
       " 'When will you do what? - anticipating temporal occurrences of activities',\n",
       " 'Rolling-unrolling lstms for action anticipation from first-person video',\n",
       " 'Anticipative video transformer',\n",
       " 'Summarization of egocentric videos: A comprehensive survey',\n",
       " 'Story-driven summarization for egocentric video',\n",
       " 'Epic-kitchens visor benchmark: Video segmentations and object relations',\n",
       " 'Forecasting human object interaction: Joint prediction of motor attention and egocentric activity',\n",
       " 'Seeing invisible poses: Estimating 3d body pose from egocentric video',\n",
       " 'Personal object discovery in first-person videos',\n",
       " 'Is first person vision challenging for object tracking?',\n",
       " 'Visual object tracking in first person vision']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_references_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cc079-f8a5-4ce6-b7bc-beeaf73ba631",
   "metadata": {},
   "source": [
    "## Access the PDFs from metadata\n",
    "\n",
    "### Observations\n",
    "\n",
    "* When a DoI is present, open access button is a good API to get the pdf url from DOI. However, it is not perfect.\n",
    "* Open access pdf search is integrated directly into semantic scholar. This sometimes gets the pdf. If it is an arxiv paper, we can use the arxiv id to get the pdfs directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f203d309-1c04-4de2-83ce-ee8481fd7863",
   "metadata": {},
   "outputs": [],
   "source": [
    "for meta in matched_references_meta:\n",
    "    if meta['openAccessPdf'] is not None:\n",
    "        meta['pdf_url'] = meta['openAccessPdf']['url']\n",
    "    elif 'ArXiv' in meta['externalIds']:\n",
    "        meta['pdf_url'] = f\"https://arxiv.org/pdf/{meta['externalIds']['ArXiv']}\"\n",
    "    else:\n",
    "        meta['pdf_url'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67d4760b-ba5b-4bcd-9eec-c2e0574c4af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://arxiv.org/pdf/1804.02748',\n",
       " 'https://arxiv.org/pdf/2110.07058',\n",
       " 'http://vision.cs.utexas.edu/projects/egocentric/egocentric_cvpr2012.pdf',\n",
       " None,\n",
       " 'http://repository.gatech.edu/bitstreams/4f49dbef-a8e1-44f8-842b-72a7370d1751/download',\n",
       " 'https://arxiv.org/pdf/1908.08498',\n",
       " 'https://arxiv.org/pdf/2104.07905',\n",
       " None,\n",
       " 'https://arxiv.org/pdf/1905.12681',\n",
       " 'https://arxiv.org/pdf/1905.00561',\n",
       " 'https://arxiv.org/pdf/1804.00892',\n",
       " 'https://arxiv.org/pdf/2005.02190',\n",
       " 'https://arxiv.org/pdf/2106.02036',\n",
       " None,\n",
       " 'https://www.cs.utexas.edu/~grauman/papers/lu-grauman-cvpr2013.pdf',\n",
       " 'http://arxiv.org/pdf/2209.13064',\n",
       " 'https://arxiv.org/pdf/1911.10967',\n",
       " 'https://arxiv.org/pdf/1603.07763',\n",
       " None,\n",
       " 'https://arxiv.org/pdf/2108.13665',\n",
       " 'https://link.springer.com/content/pdf/10.1007/s11263-022-01694-6.pdf']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m['pdf_url'] for m in matched_references_meta]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b3d0e-22f3-47a3-b56e-857d068368e6",
   "metadata": {},
   "source": [
    "## Download the PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3655426f-7d7d-46b3-a97a-6cd72152fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "paperdir = '/home/surya/NEU/CS5100 FAI/Project/pdfreader/python/papers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92f58d98-7b2e-4a37-ba0a-9867bd1509e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [01:05<00:00,  5.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for meta in tqdm(matched_references_meta):\n",
    "    paperId = meta['paperId']\n",
    "    pdf = meta['pdf_url']\n",
    "\n",
    "    if pdf is None:\n",
    "        continue        \n",
    "\n",
    "    file = Path(f\"{paperdir}/{paperId}.pdf\")\n",
    "    \n",
    "    # download\n",
    "    response = requests.get(pdf)\n",
    "    file.write_bytes(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9089f4-aad8-4724-ab81-1c55aaa83cdd",
   "metadata": {},
   "source": [
    "# Problems\n",
    "\n",
    "Selection across paragraphs, pages.\n",
    "\n",
    "Paragraphs broken by images and tables.\n",
    "\n",
    "~~Above problems require using multiple block matches, right now only using the top match.~~\n",
    "\n",
    "Make it work for name-based citation\n",
    "\n",
    "Make it work for 2-column references?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
